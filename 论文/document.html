
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown Document</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 100%;
			height:1200;
            margin: 0 auto;
            padding: 2rem;
        }
        h1, h2, h3, h4, h5, h6 { margin-top: 1.5rem; margin-bottom: 1rem; }
        pre { background: #f6f8fa; padding: 1rem; border-radius: 6px; overflow-x: auto; }
        code { background: #f6f8fa; padding: 0.2rem 0.4rem; border-radius: 3px; }
        blockquote { border-left: 4px solid #dfe2e5; padding-left: 1rem; margin: 1rem 0; color: #6a737d; }
        table { border-collapse: collapse; width: 100%; margin: 1rem 0; }
        th, td { border: 1px solid #dfe2e5; padding: 0.5rem; text-align: left; }
        th { background: #f6f8fa; }
    </style>
</head>
<body>
<h2 id="h2-3-prgs-patch-to-region-graph-search-for-visual-place-recognition" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="3、论文：PRGS Patch-to-Region Graph Search for Visual Place Recognition" class="reference-link"></a><span class="header-link octicon octicon-link"></span>3、论文：PRGS Patch-to-Region Graph Search for Visual Place Recognition</h2><blockquote>
<p style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">从片级到区域级图的搜索（建立所有的候选图像以及查询图像之间的邻居对应关系）</p>
</blockquote>
<p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><strong>主要方法</strong>：<br>![img_13.png]  </p>
<ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">1)提出Patch-to-Region Graph Search以提高重新排序使用相邻对应的候选图像  </li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">2)设计Patch to region模型，将片级特征聚合到区域级特征来突出重点特征，进而满足搜索相邻对应关系依赖于重要特征这一需求  </li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">3)设计graph search模型来实现利用邻域对应关系来估计候选图像的重排序得分，这样可以在图空间中建立所有候选图像和查询图像之间的邻居对应关系  </li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">4)PRGS完美结合了CNN和transformer的骨干网络<br>·全局性检索通过比较输入嵌入层的全局性特征之间的距离来检索候选图像；重排列（是全局检索的后阶段）则是通过匹配查询图像和候选图像之间的局部特征<br>·现在的许多全局检索方法探索神经网络使之产生更强大的表示，由此消除重排序的必要性（比如StructVPR使用丰富的语义分割中丰富的知识结构图像来改进RGB图像中的全局特征表示）；重排列可以提高性能，也会提高算力的消耗<br>就比如最近提出的一种数据驱动模型，他的高计算开销尚待解决</li></ul>
<h4 id="h4--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="文章重点提出的问题：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>文章重点提出的问题：</h4><ul>
<li><p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">先前的方法都是在查询图像和候选图像之间建立一个的对应关系来重排序，却忽略了检索到的候选图像中存在的邻居对应关系，那么是否可以利用所有检索到的候选图像之间的邻居对应关系来增强重排列  </p>
</li><li><p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">因此提出了想法：在所有候选图像之间建立邻居对应关系，以通过局部特征的角度来增强重排序  </p>
</li></ul>
<h4 id="h4--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="核心思想：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>核心思想：</h4><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">以前方法通常只考虑候选图像和查询图像之间的单一对应关系进行重排序，而本文强调后西安图像中的重要特征，以建立所有候选图像以及查询图像之间的邻居对应关系，以重新排序，这样的相邻一致性能够帮助模型更好的区分图像是真相似还是假相似  </li></ul>
<p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><strong>创新点一</strong>：提出一种片段到区域级（Patch-to-Region）的图像搜索方法（这种方法可以聚焦于建立候选图片和查询图片之间的相邻一致性）；同时，使用片段到区域级（Patch-to-Region）模型来聚合片级特征到区域级别特征，以突出更合适搜索相邻一致性的重点特征<br><strong>创新点二</strong>：设计的图搜索（GS）模块，可以建立检索到的所有候选图像与查询图像之间的邻居对应关系【它是利用卷积网络（GCN）来构造图空间，进而使得所有候选图像都能找到和他们相似的图像】</p>
<blockquote>
<h4 id="h4--strong-strong-" style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;"><a name="<strong>前置发展</strong>：" class="reference-link"></a><span class="header-link octicon octicon-link"></span><strong>前置发展</strong>：</h4><ul>
<li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">已经有DELF和DELG这样的神经网络对特征提取有了广泛的探索</li><li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">Patch-NetVLAD 对NetVLAD的残差向量执行区域求和以获得多尺度补丁特征，打破了传统获得局部特征的方法  </li><li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">TransVPR [15]利用Transformer层来提取全局和局部特征，并为重新排序中使用的局部特征的选择生成高质量的注意力分数。  </li><li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">𝑅2Former不需要额外的参数，它直接利用共享全局特征提取模型的参数来获得局部特征关注度分数，然后，这些分数被用来选择可能包含最有用信息的局部特征，并作为重新排名的主要基础  </li></ul>
</blockquote>
<h4 id="h4--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="重排列：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>重排列：</h4><ol>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">基于几何验证的非学习重排序范式在以往的重排序体系结构中得到了广泛的应用</li><li><p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">例如：Patch-NetVLAD 和TransVPR都利用RANSAC来实现SOTA性能。</p>
</li><li><p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">基于学习的重排序架构:  </p>
</li></ol>
<blockquote>
<ul>
<li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">特征匹配领域：SuperGlue、LoFTR </li><li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">图像检索领域：CVNet、RRT （难以推广到VPR）</li><li style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">𝑅2Former 是专门为VPR设计的统一的基于transformer的架构。它将注意力值和标记的xy坐标合并到特征相关矩阵中，以增强重新排序。  </li></ul>
</blockquote>
<p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">4.（PRGS是一种专门为重排序而设计的方法）</p>
<h3 id="h3--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="流程：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>流程：</h3><ol>
<li><p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">特征表示：<br>通过特征编码器获得局部特征和相应的注意力分数，xy坐标从局部特征的相应索引中导出。这些元素连接在一起以获得混合局部特征。然后，我们根据局部特征关注度得分。PR模块迭代地聚合从块级到区域级的混合局部特征，突出重要特征</p>
</li><li><p style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">特征匹配：<br>在GS模块中，在图匹配之前对混合特征进行降维（DR）处理。</p>
</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">最后使用全连接层来计算图像分数，训练过程使用<strong>交叉熵损失
</strong>和<strong>三重态损失</strong>监督</li></ol>
<h3 id="h3--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="特征表示：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>特征表示：</h3><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">使用DeiT（the data-efficient version of ViT），对其输入图像，将图像分为PxP个块，使用<strong>连接映射层</strong>将这些块级图像转为token</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><strong>注意力分数</strong>非常重要，它是供模型图评估局部特征的重要基础要素</li></ul>
<h3 id="h3-u7279u5F81u878Du5408" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="特征融合" class="reference-link"></a><span class="header-link octicon octicon-link"></span>特征融合</h3><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">直接将局部特征的<strong>绝对位置坐标</strong>和<strong>注意力得分</strong>与<strong>局部特征</strong>相结合，沿着通道维度连接起来，获得了混合局部特征（先前多数运用的方法是只关注单个局部特征并在局部特征中隐式编码位置信息）。</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">位置坐标显示包含增强了局部特征的表示，<strong>但并不是所有的token都提供有益的信息，有的token可能包含无关信息</strong></li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">解决方法：采用<strong>注意力分数</strong>来筛选过滤与采样任务不相关的tokens</li></ul>
<h4 id="h4--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="特征匹配：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>特征匹配：</h4><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">使用GCN(图卷积神经网络)的结构</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">引入子增强的自适应算法，用于在计算邻接矩阵时增强其节点</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">使用Patch-to-Region模型将块级特征<strong>迭代的</strong>聚合到区域级特征中。<strong>特别的</strong>，先通过<strong>自匹配</strong>配合<strong>交叉匹配</strong>突出重要特征，最后通过卷积操作聚合特征</li></ul>
<blockquote>
<p style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;"><strong>自匹配</strong>：使用<strong>三层GCN</strong>，加速图像中特征间的信息交换，利于重排序。<br><strong>交叉匹配</strong>通过不同图像中特征之间的信息交互，突出候选图像中与查询图像中特征相关性高的特征<br><strong>图像匹配</strong><br>以上三种匹配都包含三层GCN  </p>
</blockquote>
<h4 id="h4-u7279u5F81u805Au5408" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="特征聚合" class="reference-link"></a><span class="header-link octicon octicon-link"></span>特征聚合</h4><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">虽然通过自匹配以及交叉匹配，但是仍然存在很多不相关的特征在低层特征图中</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">解决方法：使用卷积运算将块级特征聚合成区域级特征</li></ul>
<h4 id="h4-u56FEu50CFu641Cu7D22" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="图像搜索" class="reference-link"></a><span class="header-link octicon octicon-link"></span>图像搜索</h4><blockquote>
<p style="text-shadow: rgb(102, 102, 102) 0px 0px 0px !important;">线索小来自PR模块的会计特征，然后使用这些特征来构建图节点，这样的话每个图像都能在图空间中搜索和它相似的图像<br>激活函数：GELU<br><strong>图像匹配使得重排序不再受限于建立查询和候选图像之间的单一关联</strong>，最后使用全连接层来预测每一个候选图像的分数</p>
</blockquote>
<h3 id="h3--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="实验训练步骤：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>实验训练步骤：</h3><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">训练全局检索模型</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">冻结全局检索模型，训练重排列模型</li><li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">对两个模型进行联合微调</li></ul>
<h3 id="h3--" style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;"><a name="消融实验：" class="reference-link"></a><span class="header-link octicon octicon-link"></span>消融实验：</h3><ul>
<li style="text-shadow: rgb(51, 51, 51) 0px 0px 0px !important;">卷积层有利于在图搜索阶段增强正样本之间的特征交互</li></ul>

</body>
</html>