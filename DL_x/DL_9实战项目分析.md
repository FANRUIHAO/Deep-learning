不同项目的环境或许不同，可以根据说明下载需要的包，再anaconda里新建环境，然后在环境中下载对应的包pip install xxx

将数据拆成两份：
    一份用于验证
    一份用于训练


- 模块一：读文件，加载数据
把数据下标取反，取相反数量的数据，作为验证集
注：词表记录词对应的id，但是不可能所有的数都在词表里
    对于处理不在词表里的数的方法：
    1、直接把数字当作ids返回
    2、直接加字到词表里
        ·用分词器tokenizer读出词表
        ·vodabs获取词表
        ·调用模型  
         原因：因为加入词的原因导致词表长度改变，token enbedding（词表长度，）改变了，所以要调用模型
    3、重新制作词表(较优)
    > 里面只包含数据集涉及的词+特殊的token
    先将数据映射为ids

- 模块二：统计出现的所有的数字
    counter()
- 模块三：取出现次数在阈值以上的字，放入列表中
将没有出现过的词加入词表
-------------------------------------
自监督预训练
    在已经预训练好的模型基础上加一步预训练
     注：gpt为一种生成模型，不断预测下一个字
使用bert式预训练，输入一个词进行一个预测

- 预训练过程：读数据，模型训练，得到预测值，求loss，loss回传，验证，评分
             数据(能够参加预训练的数据)
                训练集的x，验证集的x
                因为验证集的y是用来验证的，在训练的时候是不能让模型看见，所以需要mask来把这部分数据遮盖
                    mask的过程：将要mask的数据mask变成新数据，将原来的数据作为Y，进而其他训练的数据对应的Y皆为-100.这样一来训练数据的x和y进行的loss为0，只有mask的数据进行的loss，进而既训练了数据又得到了loss
             模型
             文件pretrain加载模型，创建分词器
             penalty？？？
            
             训练:(epoch)
- 预训练完成后就可以使用预训练模型use_pre = True
    best_score为评估指标 准确率
- 微调过程(了解各模块功能):
    读数据，模型训练，得到预测值，求loss，loss回传，验证，评分
    Dataset就是用来加载数据用的而已

注：1、bart为生成模型=编码器+incodder+decodder
    训练时与用多种花样的编码
    2、为什么还要进行预训练，因为用的场景不一样，数据也是一类的，范围也不同，需要针对性数据训练
    3、评价指标：ciderD_scorer用于判断文本相似度(y and y^)即准确率
    4、optimizer优化器
    5、build_bart_inputs用于生成mask来补充为0的token
    6、targets是指模型的真实标签或者期望输出，用于指导模型。将测试结果与真实标签(targets)进行比较(loss)，来评估模型的性能

    生成任务的训练和测试是不一样的，没有targets说明要进行测试 生成；若有targets那么要进行训练

    






