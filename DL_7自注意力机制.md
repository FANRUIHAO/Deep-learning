### 如何用数据表示文字
图片是由rgb三通道表示 但是文字没法用数字表示，那么就用向量代替一个字 如：（768）等价于（768，1）都是表示长为768的向量
#### --> 编码为向量:
##### 输入（One-hot形式） Encoding
###### 模型学习过程：考虑向量为度（也就是长度） 进行全链接 链接到想要的编码的维度
每个字代表一个数 也就是维度
#### 输出：
1）可以是一个字输出一个值
2）可以是所有字结合起来输出一个值
3）输入输出长度不对应


## 输入一个字 输出一个字
    序列要考虑前后关系
    一开始定义一个向量（也就是记忆单元）通过模型每一层迭代  输出自身以及记忆单元
        但是不是所有层的输出都是复合向量需要的 如果不是需要的反而会干扰结果
        法一：用RNN（相当于传家宝一直传宗接代）（也是记忆单元） 但是RNN的弊端是不利于长序列迭代
        法二：长短期记忆（LSTM）：对向量上锁
    但是只能一个一个来，无法一次性完成所有输出结果（即串行）
**解决：自注意力机制self-attention**
        分两部分：特征提取器
                 生成模型
        transformer模型架构推动大模型架构快速发展

        self-attention其实是一个特征转换器 不会改变向量的维度
        注意力：对于不同字（向量）（特征值）有对应的注意力  对应相乘相加得到输出
#### 注意力的计算
两个特征值分别乘以各自的w即矩阵（也就是全连接）得到新的矩阵q“扫码器” and k（对应的“图片”）

然后通过q*k得到注意力α
将计算出的所有注意力通过soft-max  用soft-max公式计算出一套和为1的注意力α'
    然后将每个α*a'加和得到输出b
    为了保证结果不能是原来的矩阵要再进行一次变换 引入矩阵v（value）
也就是更改为每个v*a'加和得到输出b
也就是说b是从一句话中（包含多个字（a））得到的结果

##### 因此 对于整个自注意力过程 不同的b可以并行计算 因为他们互不影响互不依赖