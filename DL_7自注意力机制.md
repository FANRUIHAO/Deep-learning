### 一、如何用数据表示文字
图片是由rgb三通道表示 但是文字没法用数字表示,那么就用向量代替一个字 如:(768)等价于(768,1)都是表示长为768的向量
>编码为向量
##### 输入(One-hot形式) Encoding
用ONE-hot来表示字
###### 模型学习过程:考虑向量为度(也就是长度) 进行全链接 链接到想要的编码的维度
每个字代表一个数 也就是维度
#### 二、输出:
>1、可以是一个字输出一个值
2、可以是所有字结合起来输出一个值
3、输入输出长度不对应


#### 1、输入一个字 输出一个字
>序列要考虑前后关系
    一开始定义一个向量(也就是记忆单元)通过模型每一层迭代  输出自身以及记忆单元
        但不是所有层的输出都是复合向量需要的 如果不是需要的反而会干扰结果

>法一:用RNN(相当于传家宝一直传宗接代)(也是记忆单元) 但是RNN的弊端是不利于长序列迭代

>法二:长短期记忆(LSTM):对向量进行上锁
但是只能一个一个来,无法一次性完成所有输出结果(即串行)
**解决:自注意力机制self-attention**
>分两部分:
- 特征提取器
- 生成模型
        transformer模型架构推动大模型架构快速发展

###### self-attention其实是一个特征转换器 不会改变向量的维度
注意力:对于不同字(向量)(特征值)有对应的注意力,对应相乘相加得到输出
#### ⚠注意力的计算
两个特征值分别乘以各自的w即矩阵(也就是全连接)得到新的矩阵q“扫码器” and k(对应的“图片”)

然后通过q*k得到注意力α
将计算出的所有注意力通过soft-max  用soft-max公式计算出一套和为1的注意力α'
    然后将每个α*a'加和得到输出b
    为了保证结果不能是原来的矩阵要再进行一次变换 引入矩阵v(value)
也就是更改为每个v*a'加和得到输出b
也就是说b是从一句话中(包含多个字(a))得到的结果

>因此 对于整个自注意力过程 不同的b可以并行计算 因为他们互不影响互不依赖
#### 三、self-attention的过程
    1、
    各个α集合为I
    各个α所乘的q集合为Q(q=α*Wq)
    各个α所乘的k集合为K(k=α*Wk)
    各个α所乘的v集合为V(k=α*Wv)
    2、
    计算各个对应的注意力矩阵α
    3、
    然后经过soft—max得到α'
    4、
    再用α'*v 得到输出矩阵

>维度
一般一个self-attention模型 一个字的编码为度是768,字的个数可以按照128计算(主要是每一次矩阵计算要把行列对应上)

##### 多头自注意力机制
一般不会让矩阵所有长度全都一口气相乘 而是让长度分为几部分(也就是几头)
若分为四部分 那输出也就是四头

- 理论上self-attention想摞几层就摞几层,但是随之也会变得越来越复杂
- 字(矩阵)放的位置不一样不会有影响,无所谓
- 当分类时,可能把所有的特征横着加起来,作为输出特征
##### 字矩阵(词embedding)直接与位置编码(位置embadding)相加直接得到模型的输入(transformer表示)
RNN不需要位置编码(位置embadding),因为它是从头到尾一个一个按顺序计算,是串行的
##### 在实际中将向量称为token
要特别关注模型的输入输出
输入:encoder编码器 进行分类
    feed Forward就是全链接
    输出提取后的特征
>分类界中最好的模型:BERT
    BERT(特征提取器):
        任务:上游对数据进行预训练,下游进行微调
            首先在大规模文本上进行无监督预训练
            然后迁移学习进行特征提取,然后将特征加上分类头进行分类 
        
> 结构:
            embedding嵌入层
            bert layers
            pooler
        BERT的输入:也就是embading的输出
            = position embeddings+segement embeddings+ Token embeddings
            
#### BERT输出pooler:
- 法一:只输出CLS。input中cls可以直接拿来做分类,其他token直接舍弃
- 法二:平均池化。input中所有算出来的self-attention相加取平均值
- 法三:最大池化。
decoder解码器,根据提取出来的特征生成图片
    ## 生成模型界中最好的模型:GPT

